from sklearn.metrics import homogeneity_score
from sklearn.metrics import completeness_score
from sklearn.metrics import v_measure_score
from sklearn.metrics import adjusted_rand_score as ARI
from sklearn.metrics import normalized_mutual_info_score as NMI

import numpy as np
import pandas as pd
import os
import sys
import csv

# def traverse_folder_compute_metrics(root_path, save_path):
#     for filename in os.listdir(root_path):
#         if "gt_label" not in filename:
#             continue
#         pre_filename, ext = os.path.splitext(filename)
#         _, _, method_name, dataset_name = pre_filename.split("_", 3)
#
#         gt_label_path = os.path.join(root_path, filename)
#         pd_label_path = os.path.join(root_path, filename.replace("gt_label", "pd_label"))
#         gt_label = pd.read_csv(gt_label_path, index_col=0).iloc[:, 0]
#         pd_label = pd.read_csv(pd_label_path, index_col=0).iloc[:, 0]
#         print(gt_label.shape)
#         print(pd_label.shape)
#
#         metrics = compute_metrics(gt_label, pd_label)
#
#         print("{} - {}: \n {}".format(dataset_name, method_name, metrics))
#
#         # save txt
#         #save_path = os.path.join(save_path, method_name)
#         if os.path.exists(save_path) != True:
#             os.makedirs(save_path)
#         #txt_path = os.path.join(save_path, "new_metrics.txt")
#         #f = open(txt_path, "a")
#         # f.write("{} {} {}\n".format(dataset_name, ari_score, nmi_score))
#
#         #record_string = dataset_name + " " + method_name
#         #for key in metrics.keys():
#         #    record_string += " {}".format(metrics[key])
#         #record_string += "\n"
#         #f.write(record_string)
#         #f.close()
#
#         csv_path = os.path.join(save_path, "new_metrics.csv")
#         need_head = False
#         if os.path.exists(csv_path) != True:
#             need_head = True
#         f = open(csv_path, "a", encoding="utf-8â€™", newline="")
#         csv_writer = csv.writer(f)
#         record = [dataset_name, method_name]
#         head = ["dataset", "method"]
#         for key in metrics.keys():
#             record.append(metrics[key])
#             head.append(key)
#         if need_head == True:
#             csv_writer.writerow(head)
#         csv_writer.writerow(record)
#         f.close()


from tqdm import tqdm
from sklearn.metrics.cluster import *
from sklearn import metrics
from scipy.optimize import linear_sum_assignment as linear_assignment

def cluster_acc(y_true, y_pred):
    """
    Calculate clustering accuracy. Require scikit-learn installed
    # Arguments
        y: true labels, numpy.array with shape `(n_samples,)`
        y_pred: predicted labels, numpy.array with shape `(n_samples,)`
    # Return
        accuracy, in [0,1]
    """
    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    D = max(y_pred.max(), y_true.max()) + 1
    w = np.zeros((D, D), dtype=np.int64)
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1

    #from sklearn.utils.linear_assignment_ import linear_assignment
    ind = linear_assignment(w.max() - w)
    ind = np.array((ind[0], ind[1])).T
    # pdb.set_trace()
    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size


def compute_metrics(y_true, y_pred):
    acc= cluster_acc(y_true, y_pred)
    # acc=0
    f1=0
    nmi = normalized_mutual_info_score(y_true, y_pred)
    ari = adjusted_rand_score(y_true, y_pred)
    homo = homogeneity_score(y_true, y_pred)
    comp = completeness_score(y_true, y_pred)
    
    metrics = {}
    metrics["ARI"] = ari
    metrics["NMI"] = nmi
    metrics["CA"] = acc
    
    return metrics


# additional metrics for clustering.
# 1. Cluster accuracy (CA)
# def cluster_acc(y_true, y_pred):
#     """
#     Calculate clustering accuracy. Require scikit-learn installed
#     # Arguments
#         y: true labels, numpy.array with shape `(n_samples,)`
#         y_pred: predicted labels, numpy.array with shape `(n_samples,)`
#     # Return
#         accuracy, in [0,1]
#     """
#     if isinstance(y_true, pd.DataFrame):
#         y_true = y_true.values
#
#     if isinstance(y_pred, pd.DataFrame):
#         y_pred = y_pred.values
#
#     # CJY 2021.6.20 for batch effect dataset
#     #y_true = y_true.to_list()
#     y_pred = y_pred.astype(np.int64)
#
#     label_to_number = {label: number for number, label in enumerate(set(y_true))}
#     label_numerical = np.array([label_to_number[i] for i in y_true])
#
#     y_true = label_numerical.astype(np.int64)
#     assert y_pred.size == y_true.size
#     D = max(y_pred.max(), y_true.max()) + 1
#     w = np.zeros((D, D), dtype=np.int64)
#     for i in range(y_pred.size):
#         w[y_pred[i], y_true[i]] += 1
#     # from sklearn.utils.linear_assignment_ import linear_assignment
#     # ind = linear_assignment(w.max() - w)
#     # return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size
#     # https://stackoverflow.com/questions/62390517/no-module-named-sklearn-utils-linear-assignment
#     from scipy.optimize import linear_sum_assignment as linear_assignment
#     row_ind, col_ind = linear_assignment(w.max() - w)
#     return sum([w[i, j] for i, j in zip(row_ind, col_ind)]) * 1.0 / y_pred.size


# 2. Jaccard score (JS)
# Please refer to https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html
# def Jaccard_index(y_true, y_pred):
#     from sklearn.metrics.cluster import pair_confusion_matrix
#     contingency = pair_confusion_matrix(y_true, y_pred)
#     JI = contingency[1,1]/(contingency[1,1]+contingency[0,1]+contingency[1,0])
#     return JI

# 3. Homogeneity score.
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score
# Note that Homogeneity score is asysmmetric

# 4. Completeness score 
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score
# It's also asysmmetric

# 5. V measure score
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score


# if __name__=="__main__":
#     #root_path = "/home/yanhan/cjy/Single-Cell-Dataset/Single-Cell-Cluster/result/scVI"
#     #save_path = "/home/yanhan/cjy/Single-Cell-Dataset/Single-Cell-Cluster/result/"
#     #root_path = r"C:\Users\cjy\Desktop\Seurat"
#     #save_path = r"C:\Users\cjy\Desktop\"
#     if len(sys.argv) == 3:
#         root_path = sys.argv[1]
#         save_path = sys.argv[2]
#         traverse_folder_compute_metrics(root_path, save_path)
#     else:
#         raise Exception("Wrong Argv Num")