{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb6af29-5c26-4d45-846b-dd7e97bb0986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4afd8f-bcb8-4699-bf74-fc2c7d0b5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'Baron'\n",
    "met = \"scGCL_pcc\"\n",
    "d_rate = 0.1\n",
    "\n",
    "save_model_path = './'+data_name+'_'+met+str(int(d_rate*100))+'_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187fa7d8-d17e-4005-9adb-77d557751a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.cluster import *\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "\n",
    "class CellDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X)\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "\n",
    "def loader_construction(data_path):\n",
    "    data = sc.read_h5ad(data_path)\n",
    "    X_all = data.X\n",
    "    y_all = data.obs.values[:,0]\n",
    "    input_dim = X_all.shape[1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=1)\n",
    "    train_set = CellDataset(X_train, y_train)\n",
    "    test_set = CellDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=512, shuffle=True, num_workers=10)\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=512, shuffle=False, num_workers=10)\n",
    "    return train_loader, test_loader, input_dim\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    ind = np.array((ind[0], ind[1])).T\n",
    "\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    acc= cluster_acc(y_true, y_pred)\n",
    "    f1=0\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    homo = homogeneity_score(y_true, y_pred)\n",
    "    comp = completeness_score(y_true, y_pred)\n",
    "    return acc, f1, nmi, ari, homo, comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9f38b4-4a2d-4775-b743-7db62f5b1b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c862f3d-4561-4e1f-87a0-62c0434947e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './'+data_name+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72a29e4-6841-4ccf-8f91-98658d475981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, input_dim = loader_construction(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ec0358-f589-44d5-8727-6f23afc7f36f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = sc.read_h5ad(data_path)\n",
    "# X_all = data.X\n",
    "y_all = data.obs.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0639a4-9e0b-4a8d-9e21-349255d4d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_clusters = len(np.unique(y_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4be629e-3e3d-44f7-8487-f2ddc884749a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import sys\n",
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# To fix the random seed\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import os\n",
    "from utils import EMA, set_requires_grad, init_weights, update_moving_average, loss_fn, repeat_1d_tensor, currentTime\n",
    "import copy\n",
    "import pandas as pd\n",
    "from data import Dataset\n",
    "from embedder_layer import embedder\n",
    "from utils import config2string\n",
    "from embedder_layer import Encoder\n",
    "import faiss\n",
    "from ZINB_loss import ZINB,NB\n",
    "import utils\n",
    "\n",
    "# scGCL Model\n",
    "# Revised freom Original version in AFGRL\n",
    "# Ref:\n",
    "# https://github.com/Namkyeong/AFGRL/tree/master/models/AFGRL.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f549c901-7d61-4782-af94-414f0a9d18e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DropData(batch_x, d_rate):\n",
    "    zero_idx = torch.where(batch_x != 0, torch.ones(batch_x.shape).to(device),\n",
    "                           torch.zeros(batch_x.shape).to(device))\n",
    "    batch_x_nozero = torch.where(batch_x == 0, torch.zeros(batch_x.shape).to(device) - 999, batch_x)\n",
    "    sample_mask = torch.rand(batch_x_nozero.shape, device=device) <= d_rate\n",
    "    batch_x_drop = torch.where(sample_mask, torch.zeros(batch_x_nozero.shape).to(device), batch_x_nozero)\n",
    "\n",
    "    final_mask = torch.where(batch_x_drop == 0, torch.ones(batch_x_drop.shape).to(device),\n",
    "                             torch.zeros(batch_x_drop.shape).to(device)) * zero_idx\n",
    "    final_x = torch.where(batch_x_drop == -999, torch.zeros(batch_x.shape).to(device), batch_x_drop)\n",
    "\n",
    "    return final_mask, final_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a271f458-43db-4036-80f2-f2f4eea2264e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Neighbor(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Neighbor, self).__init__()\n",
    "        # self.device = args.device\n",
    "        self.device = \"cuda:0\"\n",
    "        self.num_centroids = args.num_centroids\n",
    "        self.num_kmeans = args.num_kmeans\n",
    "        self.clus_num_iters = args.clus_num_iters\n",
    "\n",
    "    def __get_close_nei_in_back(self, indices, each_k_idx, cluster_labels, back_nei_idxs, k):\n",
    "        # get which neighbors are close in the background set\n",
    "        batch_labels = cluster_labels[each_k_idx][indices]\n",
    "        top_cluster_labels = cluster_labels[each_k_idx][back_nei_idxs]\n",
    "        batch_labels = repeat_1d_tensor(batch_labels, k)\n",
    "\n",
    "        curr_close_nei = torch.eq(batch_labels, top_cluster_labels)\n",
    "        return curr_close_nei\n",
    "\n",
    "    def forward(self, adj, student, teacher, top_k, epoch):\n",
    "        n_data, d = student.shape\n",
    "        similarity = torch.matmul(student, torch.transpose(teacher, 1, 0).detach())\n",
    "        similarity += torch.eye(n_data, device=self.device).to(device) * 10\n",
    "\n",
    "        _, I_knn = similarity.topk(k=top_k, dim=1, largest=True, sorted=True)\n",
    "        tmp = torch.LongTensor(np.arange(n_data)).unsqueeze(-1).to(self.device)\n",
    "\n",
    "        knn_neighbor = self.create_sparse(I_knn)\n",
    "        locality = knn_neighbor * adj\n",
    "\n",
    "        ncentroids = self.num_centroids\n",
    "        niter = self.clus_num_iters\n",
    "\n",
    "        pred_labels = []\n",
    "        # d_means = []\n",
    "        for seed in range(self.num_kmeans):\n",
    "            kmeans = faiss.Kmeans(d, ncentroids, niter=niter, gpu=False, seed=seed + 1234)\n",
    "            kmeans.train(teacher.cpu().numpy())\n",
    "            _, I_kmeans = kmeans.index.search(teacher.cpu().numpy(), 1)\n",
    "\n",
    "            clust_labels = I_kmeans[:,0]\n",
    "            # d_means.append(D_kmeans)\n",
    "            pred_labels.append(clust_labels)\n",
    "        # d_means_s = np.stack(d_means, axis=0)\n",
    "        # d_means_s = np.mean(d_means_s,axis=0)\n",
    "        # d_means_s = torch.from_numpy(d_means_s).float()\n",
    "        # print(d_means_s.shape)\n",
    "        pred_labels = np.stack(pred_labels, axis=0)\n",
    "        cluster_labels = torch.from_numpy(pred_labels).float()\n",
    "\n",
    "        all_close_nei_in_back = None\n",
    "        with torch.no_grad():\n",
    "            for each_k_idx in range(self.num_kmeans):\n",
    "                curr_close_nei = self.__get_close_nei_in_back(tmp.squeeze(-1), each_k_idx, cluster_labels, I_knn, I_knn.shape[1])\n",
    "\n",
    "                if all_close_nei_in_back is None:\n",
    "                    all_close_nei_in_back = curr_close_nei\n",
    "                else:\n",
    "                    all_close_nei_in_back = all_close_nei_in_back | curr_close_nei\n",
    "\n",
    "        all_close_nei_in_back = all_close_nei_in_back.to(self.device)\n",
    "\n",
    "        globality = self.create_sparse_revised(I_knn, all_close_nei_in_back)\n",
    "\n",
    "        pos_ = locality + globality\n",
    "\n",
    "        return pos_.coalesce()._indices(), I_knn.shape[1]\n",
    "\n",
    "    def create_sparse(self, I):\n",
    "        \n",
    "        similar = I.reshape(-1).tolist()\n",
    "        index = np.repeat(range(I.shape[0]), I.shape[1])\n",
    "        \n",
    "        assert len(similar) == len(index)\n",
    "        indices = torch.tensor([index, similar],dtype=torch.int32).to(self.device)\n",
    "        result = torch.sparse_coo_tensor(indices, torch.ones_like(I.reshape(-1)), [I.shape[0], I.shape[0]])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def create_sparse_revised(self, I, all_close_nei_in_back):\n",
    "        n_data, k = I.shape[0], I.shape[1]\n",
    "\n",
    "        index = []\n",
    "        similar = []\n",
    "        for j in range(I.shape[0]):\n",
    "            for i in range(k):\n",
    "                index.append(int(j))\n",
    "                similar.append(I[j][i].item())\n",
    "\n",
    "        index = torch.masked_select(torch.LongTensor(index).to(self.device), all_close_nei_in_back.reshape(-1))\n",
    "        similar = torch.masked_select(torch.LongTensor(similar).to(self.device), all_close_nei_in_back.reshape(-1))\n",
    "\n",
    "        assert len(similar) == len(index)\n",
    "        indices = torch.tensor([index.cpu().numpy().tolist(), similar.cpu().numpy().tolist()]).to(self.device)\n",
    "        result = torch.sparse_coo_tensor(indices, torch.ones(len(index)).to(self.device), [n_data, n_data])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23805ef3-2cbf-415f-bfab-7483eff090d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AFGRL(nn.Module):\n",
    "    def __init__(self, layer_config, args, **kwargs):\n",
    "        super().__init__()\n",
    "        dec_dim = [512, 256]\n",
    "        self.student_encoder = Encoder(layer_config=layer_config, dropout=args.dropout, **kwargs)\n",
    "        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n",
    "        set_requires_grad(self.teacher_encoder, False)\n",
    "        self.teacher_ema_updater = EMA(args.mad, args.epochs)\n",
    "        self.neighbor = Neighbor(args)\n",
    "        rep_dim = layer_config[-1]\n",
    "        rep_dim_o = layer_config[0]\n",
    "        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, args.pred_hid), nn.BatchNorm1d(args.pred_hid), nn.ReLU(), nn.Linear(args.pred_hid, rep_dim), nn.ReLU())\n",
    "        self.ZINB_Encoder = nn.Sequential(nn.Linear(rep_dim, dec_dim[0]), nn.ReLU(),\n",
    "                                          nn.Linear(dec_dim[0], dec_dim[1]), nn.ReLU())\n",
    "        self.pi_Encoder =  nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o),nn.Sigmoid())\n",
    "        self.disp_Encoder = nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o), nn.Softplus())\n",
    "        self.mean_Encoder = nn.Linear(dec_dim[1], rep_dim_o)\n",
    "        self.student_predictor.apply(init_weights)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.topk = args.topk\n",
    "        # self._device = args.device\n",
    "        self._device = \"cpu\"\n",
    "    def clip_by_tensor(self,t, t_min, t_max):\n",
    "        \"\"\"\n",
    "        clip_by_tensor\n",
    "        :param t: tensor\n",
    "        :param t_min: min\n",
    "        :param t_max: max\n",
    "        :return: cliped tensor\n",
    "        \"\"\"\n",
    "        t = torch.tensor(t,dtype = torch.float32)\n",
    "        t_min = torch.tensor(t_min,dtype = torch.float32)\n",
    "        t_max = torch.tensor(t_max,dtype = torch.float32)\n",
    "\n",
    "        result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n",
    "        result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n",
    "        return result\n",
    "\n",
    "    def reset_moving_average(self):\n",
    "        del self.teacher_encoder\n",
    "        self.teacher_encoder = None\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n",
    "        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n",
    "\n",
    "    def forward(self, x, y, edge_index, neighbor, edge_weight=None, epoch=None):\n",
    "        student = self.student_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "        # student_ = self.student_encoder(x=x2, edge_index=edge_index_2, edge_weight=edge_weight_2)\n",
    "        pred = self.student_predictor(student)\n",
    "        # pred_ = self.student_predictor(student_)\n",
    "        z = self.ZINB_Encoder(student)\n",
    "        pi = self.pi_Encoder(z)\n",
    "        disp = self.disp_Encoder(z)\n",
    "        disp = self.clip_by_tensor(disp,1e-4,1e4)\n",
    "        mean = self.mean_Encoder(z)\n",
    "        mean = self.clip_by_tensor(torch.exp(mean),1e-5,1e6)\n",
    "        modify = 0\n",
    "        with torch.no_grad():\n",
    "            teacher = self.teacher_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "            # teacher_ = self.teacher_encoder(x=x2, edge_index=edge_index_2, edge_weight=edge_weight_2)\n",
    "        if edge_weight == None:\n",
    "            adj = torch.sparse.FloatTensor(neighbor[0], torch.ones_like(neighbor[0][0]), [x.shape[0], x.shape[0]])\n",
    "        else:\n",
    "            adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])\n",
    "        #\n",
    "        ind, k = self.neighbor(adj, F.normalize(student, dim=-1, p=2), F.normalize(teacher, dim=-1, p=2), self.topk, epoch)\n",
    "        zinb = ZINB(pi, theta=disp, ridge_lambda=0, debug=False)\n",
    "        zinb_loss = zinb.loss(x, mean, mean=True)\n",
    "        # adj_recon = torch.matmul(z,z.T)\n",
    "        loss1 = loss_fn(pred[ind[0]], teacher[ind[1]].detach())\n",
    "        loss2 = loss_fn(pred[ind[1]], teacher[ind[0]].detach())\n",
    "        # loss1 = loss_fn(pred, teacher_.detach())\n",
    "        # loss2 = loss_fn(pred_, teacher.detach())\n",
    "        recon_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        recon_loss_ = recon_loss(x,student)\n",
    "        # adj_recon_ = recon_loss(adj.to_dense(),adj_recon)\n",
    "        loss_reforce = (loss1 + loss2)\n",
    "        if modify == 0:\n",
    "            loss = zinb_loss + loss_reforce + recon_loss_\n",
    "        elif modify == 1:\n",
    "            loss = loss_reforce + recon_loss_\n",
    "        elif modify == 2:\n",
    "            loss = zinb_loss\n",
    "        return student, loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e73e869-9e85-4a39-afd2-d31158635590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--embedder\", type=str, default=\"AFGRL\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"adam\", help=\"Name of the dataset. Supported names are: wikics, cs, computers, photo, and physics\")\n",
    "\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default = './model_checkpoints', help='directory to save checkpoint')\n",
    "    parser.add_argument(\"--root\", type=str, default=\"data\")\n",
    "    parser.add_argument(\"--task\", type=str, default=\"clustering\", help=\"Downstream task. Supported tasks are: node, clustering, similarity\")\n",
    "    \n",
    "    parser.add_argument(\"--layers\", nargs='?', default='[2048]', help=\"The number of units of each layer of the GNN. Default is [256]\")\n",
    "    parser.add_argument(\"--pred_hid\", type=int, default=2048, help=\"The number of hidden units of layer of the predictor. Default is 512\")\n",
    "    \n",
    "    parser.add_argument(\"--topk\", type=int, default=4, help=\"The number of neighbors to search\")\n",
    "    parser.add_argument(\"--clus_num_iters\", type=int, default=20)\n",
    "    parser.add_argument(\"--num_centroids\", type=int, default=9, help=\"The number of centroids for K-means Clustering\")\n",
    "    parser.add_argument(\"--num_kmeans\", type=int, default=5, help=\"The number of K-means Clustering for being robust to randomness\")\n",
    "    parser.add_argument(\"--eval_freq\", type=float, default=5, help=\"The frequency of model evaluation\")\n",
    "    parser.add_argument(\"--mad\", type=float, default=0.9, help=\"Moving Average Decay for Teacher Network\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")    \n",
    "    parser.add_argument(\"--es\", type=int, default=300, help=\"Early Stopping Criterion\")\n",
    "    parser.add_argument(\"--device\", type=int, default=0)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=300)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--aug_params\", \"-p\", nargs=\"+\", default=[0.3, 0.4, 0.3, 0.2],help=\"Hyperparameters for augmentation (p_f1, p_f2, p_e1, p_e2). Default is [0.2, 0.1, 0.2, 0.3]\")\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b394f813-0708-4f4e-90b9-cc2d10bee6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1511f54f-35b1-40d1-a971-17623201b4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26399f92-a93c-49e9-9753-03a772cd2d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layer_config, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.afgrl = AFGRL(layer_config, args)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        data_normalized = x.cpu().detach().numpy()\n",
    "        similarity_matrix = cosine_similarity(data_normalized)\n",
    "\n",
    "        k = 15  # 设置k值\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(data_normalized)\n",
    "        distances, indices = nbrs.kneighbors(data_normalized)\n",
    "\n",
    "        edges = []\n",
    "        for i in range(indices.shape[0]):\n",
    "            for j in range(1, k+1):  \n",
    "                edges.append((i, indices[i, j]))\n",
    "\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(device)\n",
    "        neighbor = [edge_index,]\n",
    "        x_hat, loss = self.afgrl(x, y, edge_index, neighbor, edge_weight=None, epoch=None)\n",
    "        \n",
    "        return x_hat, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9267f8cc-9172-426d-aafe-3a2f85e2daa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def l1_distance(imputed_data, original_data):\n",
    "\n",
    "    return np.mean(np.abs(original_data-imputed_data))\n",
    "\n",
    "def RMSE(imputed_data, original_data):\n",
    "    return np.sqrt(np.mean((original_data - imputed_data)**2))\n",
    "\n",
    "\n",
    "def pearson_corr(imputed_data, original_data):\n",
    "    Y = original_data\n",
    "    fake_Y = imputed_data\n",
    "    fake_Y, Y = fake_Y.reshape(-1), Y.reshape(-1)\n",
    "    fake_Y_mean, Y_mean = np.mean(fake_Y), np.mean(Y)\n",
    "    corr = (np.sum((fake_Y - fake_Y_mean) * (Y - Y_mean))) / (\n",
    "            np.sqrt(np.sum((fake_Y - fake_Y_mean) ** 2)) * np.sqrt(np.sum((Y - Y_mean) ** 2)))\n",
    "    return corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce18d0a1-2b80-40a6-80ce-fc9ac5a1d483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24a0ed43-040c-481c-b583-a36453b14da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_loader,\n",
    "          test_loader,\n",
    "          input_dim,\n",
    "          lr,\n",
    "          seed,\n",
    "          epochs,\n",
    "          device):\n",
    "    \n",
    "    layer_config = [input_dim] + [input_dim//2,input_dim//4,input_dim//2,input_dim] \n",
    "    model = Model(layer_config, args).to(device)\n",
    "\n",
    "    opt_model = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    setup_seed(seed)\n",
    "    train_loss = []\n",
    "    min_loss = 99999\n",
    "    best_epoch = 0\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    np.set_printoptions(precision=2)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    x_imp_list = []\n",
    "    for each_epoch in range(epochs):\n",
    "        batch_loss = []\n",
    "        model.train()\n",
    "\n",
    "        for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            final_mask, final_x = DropData(batch_x, d_rate)\n",
    "\n",
    "            x_imp, loss = model(final_x, batch_y)\n",
    "            \n",
    "            opt_model.zero_grad()\n",
    "            loss.backward()\n",
    "            opt_model.step()\n",
    "\n",
    "            batch_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        train_loss.append(np.mean(np.array(batch_loss)))\n",
    "        with torch.no_grad():\n",
    "            batch_x_imp = []\n",
    "            batch_loss = []\n",
    "            model.eval()\n",
    "\n",
    "            for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "                final_mask, final_x = DropData(batch_x, d_rate)\n",
    "\n",
    "                x_imp, loss = model(final_x, batch_y)\n",
    "                batch_x_imp.append(x_imp)\n",
    "                batch_loss.append(loss.cpu().detach().numpy())\n",
    "         \n",
    "        x_imp_list.append(torch.cat(batch_x_imp).cpu().detach().numpy())\n",
    "        cur_loss = np.mean(np.array(batch_loss))\n",
    "        if cur_loss < min_loss:\n",
    "            min_loss = cur_loss\n",
    "            best_epoch = each_epoch\n",
    "            \n",
    "    return np.array(x_imp_list[best_epoch])\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d769021-93fa-4b05-85a2-edea376711ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_imp = train(train_loader, test_loader, input_dim, lr=0.0001, seed=1, epochs=200, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
