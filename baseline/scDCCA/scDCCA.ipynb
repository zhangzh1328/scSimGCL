{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3345bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'Baron'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2a2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from typing import Optional\n",
    "from sklearn import metrics\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e90b563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7b38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from layers import ZINBLoss, MeanAct, DispAct\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import math, os\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import scanpy as sp\n",
    "from evaluation import evaluate\n",
    "from preprocess import *  ####用于导入指定模块中的全部定义。\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from contrastive_loss import ClusterLoss, InstanceLoss\n",
    "from pandas import Series\n",
    "from time import time as get_time\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117a5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_info():\n",
    "    #计算消耗内存\n",
    "    pid = os.getpid()\n",
    "    # 模块名比较容易理解：获得当前进程的pid\n",
    "    p = psutil.Process(pid)\n",
    "    # 根据pid找到进程，进而找到占用的内存值\n",
    "    info = p.memory_full_info()\n",
    "    memory = info.uss / 1024 / 1024\n",
    "    return memory\n",
    "\n",
    "\n",
    "def buildNetwork(layers, activation=\"relu\"):\n",
    "    net = []\n",
    "    for i in range(1, len(layers)):\n",
    "        net.append(nn.Linear(layers[i-1], layers[i]))\n",
    "        if activation==\"relu\":\n",
    "            net.append(nn.ReLU())\n",
    "        elif activation==\"sigmoid\":\n",
    "            net.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, n_clusters, encodeLayer=[], decodeLayer=[], \n",
    "            activation=\"relu\", sigma=1.0, alpha=1.0, gamma=1.0, i_temp=0.5, c_temp=1.0, i_reg=0.5, c_reg=0.2, feature_dim=32, device='cpu'):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_clusters = n_clusters\n",
    "        self.activation = activation\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.encoder = buildNetwork([input_dim]+encodeLayer, activation=activation)###2000-256-64\n",
    "        self.decoder = buildNetwork([z_dim]+decodeLayer, activation=activation)###32-64-256\n",
    "        self.decoder1 = buildNetwork([z_dim]+decodeLayer, activation=activation)###32-64-256\n",
    "        self._enc_mu = nn.Linear(encodeLayer[-1], z_dim)###64-32\n",
    "        self._dec_mean = nn.Sequential(nn.Linear(decodeLayer[-1], input_dim), MeanAct())##256-2000\n",
    "        self._dec_disp = nn.Sequential(nn.Linear(decodeLayer[-1], input_dim), DispAct())##256-2000\n",
    "        self._dec_pi = nn.Sequential(nn.Linear(decodeLayer[-1], input_dim), nn.Sigmoid())##256-2000\n",
    "        \n",
    "        self.i_temp = i_temp\n",
    "        self.c_temp = c_temp\n",
    "        self.i_reg = i_reg####0.5\n",
    "        self.c_reg = c_reg####0.2\n",
    "        \n",
    "        self.instance_projector = nn.Sequential(####实例投影\n",
    "            nn.Linear(z_dim, z_dim),###32-32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(z_dim, feature_dim))###32-2000\n",
    "        \n",
    "        self.cluster_projector = nn.Sequential(#####聚类投影，z_dim维投影到n_clusters维\n",
    "            nn.Linear(z_dim, n_clusters),####32-10\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "        self.mu = nn.Parameter(torch.Tensor(n_clusters, z_dim))###生成一个10*32的质心坐标，即每个簇的坐标都是32维\n",
    "        self.zinb_loss = ZINBLoss()\n",
    "        self.to(device)\n",
    "    \n",
    "    def soft_assign(self, z):####算z所属的软标签，即用学生分布度量z与self.mu的相似度\n",
    "        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - self.mu)**2, dim=2) / self.alpha)###对于每一个cell都有一个10维度的q,因为属于每个簇的概率不同\n",
    "        q = q**((self.alpha+1.0)/2.0)\n",
    "        q = (q.t() / torch.sum(q, dim=1)).t()\n",
    "        return q  ###对于每一个cell都有一个10维度的q,因为属于每个簇的概率不同，所以q是268*10的矩阵\n",
    "    \n",
    "    def target_distribution(self, q):###\n",
    "        p = q**2 / q.sum(0)\n",
    "        return (p.t() / p.sum(1)).t()\n",
    "    \n",
    "    def x_drop(self, x, p=0.2):\n",
    "        mask_list = [torch.rand(x.shape[1]) < p for _ in range(x.shape[0])]\n",
    "        mask = torch.vstack(mask_list)\n",
    "        new_x = x.clone()\n",
    "        new_x[mask] = 0.0\n",
    "        return new_x\n",
    "    \n",
    "    def forward(self, x):###作用是对数据进行预训练，得到相应的中间变量，z0, q是用原始的x得到的，_mean, _disp, _pi是用加了噪声的x得到的\n",
    "        h = self.encoder(x+torch.randn_like(x) * self.sigma)##将输入加噪声以后进行编码，2000-256-64\n",
    "        z = self._enc_mu(h)####64-32\n",
    "        h = self.decoder(z)####32-64-256\n",
    "        \n",
    "        h1 = self.decoder1(z)####32-64-256\n",
    "        h = (h1 + h) / 2###求平均是为了减小波动\n",
    "        \n",
    "        _mean = self._dec_mean(h)\n",
    "        _disp = self._dec_disp(h)\n",
    "        _pi = self._dec_pi(h)\n",
    "\n",
    "        h0 = self.encoder(x)##将输入直接进行编码，2000-256-64\n",
    "        z0 = self._enc_mu(h0)####64-32\n",
    "        q = self.soft_assign(z0)###直接对隐藏层z0求软分配，度量的是z0和self.mu的相似性\n",
    "        return z0, q, _mean, _disp, _pi\n",
    "    \n",
    "    \n",
    "    def calc_ssl_lossv1(self, x1, x2):\n",
    "        z1, _, _, _, _ = self.forward(x1)###将x1送到编码层2000-256-64-32得到z1\n",
    "        z2, _, _, _, _ = self.forward(x2)##将x2送到编码层2000-256-64-32得到z2\n",
    "        \n",
    "        instance_loss = InstanceLoss(x1.shape[0], self.i_temp)###x1.shape[0]=256,self.i_temp=0.5,算instance_loss\n",
    "        return instance_loss.forward(z1 ,z2)###返回instance_loss\n",
    "    \n",
    "    \n",
    "    def calc_ssl_lossv2(self, x1, x2):###计算对比过程的聚类损失，2000维\n",
    "        # _, q1, _, _, _ = self.forward(x1)\n",
    "        # _, q2, _, _, _ = self.forward(x2)\n",
    "        # cluster_loss = ClusterLoss(self.n_clusters, self.c_temp)\n",
    "        # return cluster_loss.forward(q1, q2)\n",
    "        z1, _, _, _, _ = self.forward(x1)\n",
    "        z2, _, _, _, _ = self.forward(x2)###32维\n",
    "        c1 = self.cluster_projector(z1)\n",
    "        c2 = self.cluster_projector(z2)#####10维\n",
    "        cluster_loss = ClusterLoss(self.n_clusters, self.c_temp)###定义cluster_loss的计算用ClusterLoss函数，具体去看这个函数\n",
    "        return cluster_loss.forward(c1, c2)\n",
    "    \n",
    "    \"\"\"\n",
    "    def calc_ssl_loss(self, x, p=0.2):\n",
    "        x1 = self.x_drop(x, p)\n",
    "        x2 = self.x_drop(x, p)\n",
    "        z1, _, _, _, _ = self.forward(x1)\n",
    "        z2, _, _, _, _ = self.forward(x2)\n",
    "        z1 = self.instance_projector(z1)\n",
    "        z2 = self.instance_projector(z2)\n",
    "        ssl_loss1 = InstanceLoss(x.shape[0], self.i_temp)\n",
    "        instance_loss = ssl_loss1.forward(z1, z2)\n",
    "        c1 = self.cluster_projector(z1)\n",
    "        c2 = self.cluster_projector(z2)\n",
    "        ssl_loss2 = ClusterLoss(self.n_clusters, self.c_temp)\n",
    "        cluster_loss = ssl_loss2.forward(z1, z2)\n",
    "        return instance_loss, cluster_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def encodeBatch(self, X, batch_size=256):####encodeBatch的作用是对输入进行编码得到编码的嵌入向量，输入就是原始的x，\n",
    "                                               # 每次只处理一个批次的数据，将数据按批次处理，避免超显存\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        \n",
    "        encoded = []###encoded是一个空列表\n",
    "        num = X.shape[0]####num=268即细胞数目\n",
    "        num_batch = int(math.ceil(1.0*X.shape[0]/batch_size))###每批次256，算算要多少批，这里是268/256=2\n",
    "        for batch_idx in range(num_batch):###对每一批\n",
    "            xbatch = X[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]###取该批次的数据\n",
    "            inputs = Variable(xbatch)\n",
    "            z,_, _, _, _ = self.forward(inputs)###对数据进行编码2000-256-64-32得到32维的z\n",
    "            encoded.append(z.data)###将这256个32维的数据存在encoded列表中，作为它的一个元素，回到for继续，直到所有批次存完，最后encoded列表中存放的是所有细胞的32维的编码\n",
    "\n",
    "        encoded = torch.cat(encoded, dim=0)###将encoded列表的所有元素拼接起来，成为一个(num,32)的tensor\n",
    "        return encoded\n",
    "\n",
    "    def cluster_loss(self, p, q):\n",
    "        def kld(target, pred):\n",
    "            return torch.mean(torch.sum(target*torch.log(target/(pred+1e-6)), dim=-1))####这里求了均值，所以后面会* len(inputs)\n",
    "        kldloss = kld(p, q)\n",
    "        return self.gamma*kldloss ####self.gamma=1.0\n",
    "\n",
    "    def pretrain_autoencoder(self, x, X_raw, size_factor, batch_size=256, lr=0.001, epochs=400, ae_save=True, ae_weights='AE_weights.pth.tar'):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        dataset = TensorDataset(torch.Tensor(x), torch.Tensor(X_raw), torch.Tensor(size_factor))###dataset有三个tensor,分别是2个268*2000的矩阵和1个含有268个factor\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)####每批batch_size=256个，比如268个细胞就只有两批\n",
    "        print(\"Pretraining stage\")\n",
    "        \n",
    "        # optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr, amsgrad=True)\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (x_batch, x_raw_batch, sf_batch) in enumerate(dataloader):###从dataloader取这一批的相应数据\n",
    "                x_tensor = Variable(x_batch).cuda()###数据送给GPU\n",
    "                x_raw_tensor = Variable(x_raw_batch).cuda()\n",
    "                sf_tensor = Variable(sf_batch).cuda()\n",
    "                _, _, mean_tensor, disp_tensor, pi_tensor = self.forward(x_tensor)###进入上面的85行的forward,输入就一个x_tensor，输出三个，\n",
    "                # 由于只关心后面三个输出，所以其实这里输出的是加了噪声以后的x编码解码后得到的三个参数，而不是原始的x\n",
    "                zinb_loss = self.zinb_loss(x=x_raw_tensor, mean=mean_tensor, disp=disp_tensor, pi=pi_tensor, scale_factor=sf_tensor)##算zinb损失\n",
    "                \n",
    "                x1 = self.x_drop(x_tensor, p=0.2)###数据增广\n",
    "                x2 = self.x_drop(x_tensor, p=0.2)###数据增广\n",
    "\n",
    "                instance_loss = self.calc_ssl_lossv1(x1, x2)###算instance_loss\n",
    "                instance_loss = self.i_reg * instance_loss###instance_loss*自身系数\n",
    "                # cluster_loss = self.calc_ssl_lossv2(x1, x2)\n",
    "                # cluster_loss = self.c_reg * cluster_loss\n",
    "                # instance_loss, cluster_loss = self.calc_ssl_loss(x_tensor, p=0.2)\n",
    "                # instance_loss = instance_loss * self.i_reg\n",
    "                # cluster_loss = cluster_loss * self.c_reg\n",
    "                # ssl_loss = instance_loss + cluster_loss\n",
    "                \n",
    "                loss = zinb_loss + instance_loss###整个自编码预训练的损失是用加噪声的x经过编码解码求得的三个参数和原始的X求出的zinb损失和数据增广后的x1,2经过编码求的instance_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()###\n",
    "                optimizer.step()\n",
    "                print('Pretrain epoch [{}/{}], ZINB loss:{:.4f}, Instance loss:{:.4f}'.format(batch_idx+1, epoch+1,\n",
    "                                                                                              zinb_loss.item(), \n",
    "                                                                                              instance_loss.item()))###内层循环执行一个批次后在执行下一批指导所有数据执行完，\n",
    "                                                                                         # epoch自动加1，直到50个epoch执行完,所以比如deng有268个cell，每一个epoch要输出两次\n",
    "\n",
    "        if ae_save:\n",
    "            torch.save(self.state_dict(), ae_weights)\n",
    "\n",
    "\n",
    "    def fit(self, X, X_raw, sf, y=None, lr=1., batch_size=256, num_epochs=10, save_path=''):\n",
    "        '''X: tensor data'''\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        print(\"Clustering stage\")\n",
    "        X = torch.tensor(X).cuda()\n",
    "        X_raw = torch.tensor(X_raw).cuda()\n",
    "        sf = torch.tensor(sf).cuda()\n",
    "        optimizer = optim.Adadelta(filter(lambda p: p.requires_grad, self.parameters()), lr=lr, rho=.95)\n",
    "\n",
    "        print(\"Initializing cluster centers with kmeans.\")\n",
    "        kmeans = KMeans(self.n_clusters, n_init=20)###定义聚类方法是kmeans，类别数目=10,选20次最优的\n",
    "        data = self.encodeBatch(X)####data就是X编码以后的num个32维的隐层表示，是一个tensor\n",
    "        self.y_pred = kmeans.fit_predict(data.data.cpu().numpy())###对这个编码以后的隐层数据用kmeans聚类得到预测标签\n",
    "        self.y_pred_last = self.y_pred###备份一下kmeans聚类得到预测标签\n",
    "        self.mu.data.copy_(torch.Tensor(kmeans.cluster_centers_))###kmeans.cluster_centers_是self.n_clusters*32的数组，就是质心坐标,将其备份一下，放在 self.mu.data里\n",
    "        if y is not None:\n",
    "            acc, f1, nmi, ari, homo, comp = evaluate(y, self.y_pred)\n",
    "            print('Initializing k-means: ACC= %.4f, F1= %.4f, NMI= %.4f, ARI= %.4f, HOMO= %.4f, COMP= %.4f' % (acc, f1, nmi, ari, homo, comp))\n",
    "        \n",
    "        self.train()\n",
    "        num = X.shape[0]###细胞数\n",
    "        num_batch = int(math.ceil(1.0*X.shape[0]/batch_size))###算下需要多少批次\n",
    "        acc, nmi, ari, homo,comp, epoch = 0, 0, 0, 0, 0, 0\n",
    "        lst = []  #####创建一个空列表，用于存放指标\n",
    "        pred = []  #####创建一个空列表，用于存放预测标签\n",
    "        best_ari = 0.0  ###初始化最优的ARI为0\n",
    "        for epoch in range(num_epochs):\n",
    "            # update the targe distribution p\n",
    "            latent = self.encodeBatch(X)###先不考虑损失，直接对X进行编码得到隐层，268个32维的，根据这个编码的向量可以得到最初的q,p，以及用q的预测标签，和该标签的准确度等等\n",
    "            q = self.soft_assign(latent)###对编码得到的隐层向量算其与随机初始化的质心坐标的相似程度，268个\n",
    "            p = self.target_distribution(q).data###算目标分布，这里要事先把p算好，因为forward只返回q,如果这里不计算，就要在265行得到qbatch以后计算，效果是一样的\n",
    "\n",
    "            # evalute the clustering performance\n",
    "            self.y_pred = torch.argmax(q, dim=1).data.cpu().numpy()###用软标签q得到预测标签\n",
    "            acc, f1, nmi, ari, homo, comp = evaluate(y, self.y_pred)##算软标签q的指标\n",
    "\n",
    "            lab_ypred = np.unique(self.y_pred)\n",
    "            print(lab_ypred)\n",
    "\n",
    "\n",
    "\n",
    "            print('Cluster %d : ACC= %.4f, F1= %.4f, NMI= %.4f, ARI= %.4f, HOMO= %.4f, COMP= %.4f' % (epoch+1, acc, f1, nmi, ari, homo, comp))\n",
    "            pred.append(self.y_pred)  #####在列表中增加元素，只不过这个元素是每一次预测的标签\n",
    "            zhibiao = (acc, f1, nmi, ari, homo, comp)\n",
    "            lst.append(zhibiao)  #####在列表中增加元素，只不过这个元素是上面的5个指标\n",
    "\n",
    "            if best_ari < ari:###如果当前得到的ari比最优的ari大，说明当前的更好，就把当前的存起来，最终保存的是训练次数中最优的ari\n",
    "                best_ari = ari\n",
    "                torch.save({'latent': latent, 'q': q, 'p': p}, save_path)###保存隐层变量\n",
    "                latent = latent.cpu().numpy()###先从tensor转成array\n",
    "                #\n",
    "                print('save_successful')\n",
    "\n",
    "\n",
    "            # train 1 epoch for clustering loss\n",
    "            train_loss = 0.0\n",
    "            recon_loss_val = 0.0\n",
    "            cluster_loss_val = 0.0\n",
    "            c_loss_val = 0.0\n",
    "            for batch_idx in range(num_batch):\n",
    "                xbatch = X[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                xrawbatch = X_raw[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                sfbatch = sf[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                pbatch = p[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]#p是268*10的矩阵，从里面取一个批次256个\n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(xbatch)\n",
    "                rawinputs = Variable(xrawbatch)\n",
    "                sfinputs = Variable(sfbatch)\n",
    "                target = Variable(pbatch)\n",
    "\n",
    "                z, qbatch, meanbatch, dispbatch, pibatch = self.forward(inputs)###对该批次的输入进行编码得到相应的5个中间变量\n",
    "\n",
    "                inputs1 = self.x_drop(inputs, p=0.2)\n",
    "                inputs2 = self.x_drop(inputs, p=0.2)\n",
    "                c_loss = self.calc_ssl_lossv2(inputs1, inputs2)###计算编码后的对比聚类损失\n",
    "                c_loss = self.c_reg * c_loss### 对比损失乘以系数\n",
    "                \n",
    "            \n",
    "                cluster_loss = self.cluster_loss(target, qbatch)###算聚类kl损失\n",
    "                recon_loss = self.zinb_loss(rawinputs, meanbatch, dispbatch, pibatch, sfinputs)###算重构的zinb损失\n",
    "                loss = cluster_loss + recon_loss + c_loss#####总的训练损失是kl聚类损失+重构损失+对比聚类损失\n",
    "                #loss = cluster_loss + c_loss  #####总的训练损失是kl聚类损失+重构损失+对比聚类损失\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                cluster_loss_val += cluster_loss.data * len(inputs)###因为再算每一个损失的时候算的都是平均值，所以乘以个数\n",
    "                recon_loss_val += recon_loss.data * len(inputs)\n",
    "                c_loss_val += c_loss.data * len(inputs)\n",
    "                train_loss = cluster_loss_val + recon_loss_val + c_loss_val####总的训练损失是kl聚类损失+重构损失+对比聚类损失\n",
    "\n",
    "            print(\"#Epoch %3d: Total: %.4f Clustering Loss: %.4f ZINB Loss: %.4f C Loss: %.4f\" % (\n",
    "                epoch + 1, train_loss / num, cluster_loss_val / num, recon_loss_val / num, c_loss_val / num))####但是输出的这里其实还是均值\n",
    "\n",
    "        cunari = []  #####初始化\n",
    "        for j in range(len(lst)):  ###j从0到num_epochs-1\n",
    "            aris = lst[j][2]\n",
    "            cunari.append(aris)\n",
    "        max_ari = max(cunari)  ###找到最大的ari\n",
    "        maxid = cunari.index(max_ari)  ####找到最大的ari的指标\n",
    "        optimal_pred = pred[maxid]\n",
    "        #np.savetxt(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\11\\\\%s_predlabel1.csv\"% data_name, optimal_pred, delimiter=' ')###一次才用\n",
    "        final_acc, final_f1, final_nmi, final_ari, final_homo, final_comp = evaluate(y, optimal_pred)\n",
    "        return final_acc, final_f1, final_nmi, final_ari, final_homo, final_comp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adb9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(adata, HVG=0.2, filter_min_counts=True, size_factors=True, logtrans_input=True, normalize_input=True):\n",
    "\n",
    "    if filter_min_counts:\n",
    "        sc.pp.filter_genes(adata, min_counts=1)\n",
    "        sc.pp.filter_cells(adata, min_counts=1)\n",
    "\n",
    "    n = int(adata.X.shape[1] * HVG)\n",
    "    hvg_gene_idx = np.argsort(adata.X.var(axis=0))[-n:]\n",
    "    adata = adata[:,hvg_gene_idx]\n",
    "\n",
    "    adata.raw = adata.copy()\n",
    "\n",
    "    if size_factors:\n",
    "        sc.pp.normalize_per_cell(adata)\n",
    "        adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n",
    "    else:\n",
    "        adata.obs['size_factors'] = 1.0\n",
    "\n",
    "    if logtrans_input:\n",
    "        sc.pp.log1p(adata)\n",
    "\n",
    "    if normalize_input:\n",
    "        sc.pp.scale(adata)\n",
    "\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abe1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./Baron.h5'\n",
    "adata = sc.read_h5ad(path)\n",
    "\n",
    "y_all = adata.obs.values[:,0]\n",
    "\n",
    "y = y_all\n",
    "\n",
    "adata.X = adata.X.astype(np.float32)\n",
    "adata = normalize(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa144ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "input_size = adata.n_vars \n",
    "n_clusters = adata.obs['Group'].unique().shape[0]\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5ad868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Baron_pretrain_param.pth\n",
      "./Baron_param.pth\n"
     ]
    }
   ],
   "source": [
    "pretrain_path = './%s_pretrain_param.pth' % data_name\n",
    "model_path = './%s_param.pth' % data_name\n",
    "print(pretrain_path)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aaebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle = 1\n",
    "arii = np.array([])\n",
    "nmii = np.array([])\n",
    "f11 = np.array([])\n",
    "accc = np.array([])\n",
    "homoo = np.array([])\n",
    "compp = np.array([])\n",
    "\n",
    "for i in range(cycle):\n",
    "    print(\"第%d次循环\", i)\n",
    "\n",
    "    model = MyModel(\n",
    "     input_dim=input_size,###2000\n",
    "     z_dim=32,\n",
    "     n_clusters=n_clusters,\n",
    "     encodeLayer=[256, 64],\n",
    "     decodeLayer=[64,256],\n",
    "     activation='relu',\n",
    "     sigma=2.5,\n",
    "     alpha=1.0,\n",
    "     gamma=1.0,\n",
    "     device=device)\n",
    "\n",
    "    model.pretrain_autoencoder(\n",
    "     x=adata.X,\n",
    "     X_raw=adata.raw.X,\n",
    "     size_factor=adata.obs.size_factors,\n",
    "     batch_size=1024,\n",
    "     epochs=200,\n",
    "     ae_weights=pretrain_path)\n",
    "\n",
    "    final_acc, final_f1, final_nmi, final_ari, final_homo, final_comp = model.fit(\n",
    "      X=adata.X,\n",
    "      X_raw=adata.raw.X,\n",
    "      sf=adata.obs.size_factors,\n",
    "      y=y,\n",
    "      lr=1.0,\n",
    "      batch_size=1024,\n",
    "      num_epochs=200,\n",
    "      save_path=model_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    accc = np.append(accc, final_acc)\n",
    "    f11 = np.append(f11, final_f1)\n",
    "    arii = np.append(arii, final_ari)\n",
    "    nmii = np.append(nmii, final_nmi)\n",
    "    homoo = np.append(homoo, final_homo)\n",
    "    compp = np.append(compp, final_comp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
